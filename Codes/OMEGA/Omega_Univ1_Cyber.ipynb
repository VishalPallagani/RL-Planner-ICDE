{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 3453,
     "status": "ok",
     "timestamp": 1601395796616,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "OhkfI-d8utGf",
    "outputId": "e19e1779-58f1-44d1-fea6-a3ff942d1bb7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random \n",
    "import re\n",
    "from scipy import spatial\n",
    "%matplotlib inline\n",
    "import math \n",
    "from collections import Counter \n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import itertools\n",
    "from itertools import permutations \n",
    "from operator import itemgetter \n",
    "nltk.download('wordnet')\n",
    "from scipy.spatial import distance\n",
    "import sys\n",
    "#importing stopwords using nltk library\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "#creating tokens\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import itertools as it\n",
    "import time\n",
    "import random\n",
    "import pickle as pk\n",
    "import multiprocessing\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 1187,
     "status": "ok",
     "timestamp": 1601395823864,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "B-z34gYevfed",
    "outputId": "4a3c76be-b535-44ab-dd89-2b43ddbe139e"
   },
   "outputs": [],
   "source": [
    "#Univ 1 graduate courses\n",
    "Univ1_dataset=pd.read_csv('Univ-1.csv')\n",
    "Univ1_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 813,
     "status": "ok",
     "timestamp": 1601395847401,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "JekneZx7vo84"
   },
   "outputs": [],
   "source": [
    "Univ1_dataset=Univ1_dataset.applymap(str)\n",
    "#removing the non-ascii characters present in the course and prerequisite column on university dataset\n",
    "Univ1_dataset['Course']=Univ1_dataset['Course'].str.replace('\\xa0',' ')\n",
    "Univ1_dataset['Prerequisite']=Univ1_dataset['Prerequisite'].str.replace('\\xa0',' ')\n",
    "\n",
    "#removing the quotes and backslash from the course column of the dataset\n",
    "def quotes(text):\n",
    "  text = text.strip('\\\"')\n",
    "  text=text.strip()\n",
    "  return text\n",
    "Univ1_dataset['Course']=Univ1_dataset['Course'].apply(quotes)\n",
    "\n",
    "#tokenizing the prerquisites\n",
    "\n",
    "Univ1_dataset['Prerequisite'] = Univ1_dataset['Prerequisite'].str.split(',')\n",
    "\n",
    "#removing extra spaces in the pre list\n",
    "Univ1_dataset['Prerequisite'] = Univ1_dataset['Prerequisite'].apply(lambda x:[w.lstrip() for w in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1044,
     "status": "ok",
     "timestamp": 1601395851211,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "4aIpfudov0r8"
   },
   "outputs": [],
   "source": [
    "#creating courses list and prerequsites list from the Univ1_dataset\n",
    "course=list(Univ1_dataset['Course'])\n",
    "pre=list(Univ1_dataset['Prerequisite'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1601395852334,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "lEWf2IAPv3l9"
   },
   "outputs": [],
   "source": [
    "#removing the prereq which are not university-1 graduate courses\n",
    "\n",
    "for i in range(len(pre)):\n",
    "  list_1=pre[i]\n",
    "  for index,j in enumerate(list_1):\n",
    "    if j not in course:\n",
    "      list_1[index]='nan'\n",
    "\n",
    "#removing the 'nan' values\n",
    "for i in range(len(pre)):\n",
    "  list_1=pre[i]\n",
    "  for index,j in enumerate(list_1):\n",
    "    if j=='nan':\n",
    "      list_1.remove('nan')\n",
    "for i in range(len(pre)):\n",
    "  list_1=pre[i]\n",
    "  for index,j in enumerate(list_1):\n",
    "    if j=='nan':\n",
    "      list_1.remove('nan')\n",
    "for i in range(len(pre)):\n",
    "  list_1=pre[i]\n",
    "  for index,j in enumerate(list_1):\n",
    "    if j=='nan':\n",
    "      list_1.remove('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1116,
     "status": "ok",
     "timestamp": 1601395855384,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "Mh2hKm9Pv8po"
   },
   "outputs": [],
   "source": [
    "#data dict represents key as course from university-1 graduate courses and its prerequisite as value\n",
    "data_dict={}\n",
    "for i in range(len(course)):\n",
    "  data_dict[course[i]]=pre[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1601395856386,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "mu6rtX3NwBM1"
   },
   "outputs": [],
   "source": [
    "#removing redundancies in the data dictionary\n",
    "data_dict['BNFO 700B']=[]\n",
    "data_dict['BNFO 701B']=[]\n",
    "data_dict['IS 701B']=[]\n",
    "data_dict['CS 735']=['CS 675', 'CS 634', 'CS 643', 'CS 644']\n",
    "data_dict['CS 726']=[]\n",
    "data_dict['CS 700B']=[]\n",
    "data_dict['CS 635']=['CS 505','CS 510']\n",
    "data_dict['CS 640']=['CS 540']\n",
    "data_dict['CS 646']=['CS 645','CS 646','CS 696']\n",
    "data_dict['CS 650']=['CS 510']\n",
    "data_dict['CS 668']=['CS 610','CS 650']\n",
    "data_dict['CS 696']=['CS 652','CS 656']\n",
    "data_dict['CS 701C']=[]\n",
    "data_dict['CS 725']=[]\n",
    "data_dict['CS 790A']=['CS 791']\n",
    "data_dict['CS 790B']=['CS 791']\n",
    "data_dict['CS 790C']=['CS 791']\n",
    "data_dict['IS 591']=[]\n",
    "data_dict['IS 592']=[]\n",
    "data_dict['IS 593']=[]\n",
    "data_dict['IS 601']=[]\n",
    "data_dict['IS 612']=[]\n",
    "data_dict['IS 613']=[]\n",
    "data_dict['IS 614']=[]\n",
    "data_dict['IS 616']=[]\n",
    "data_dict['IS 650']=[]\n",
    "data_dict['IS 661']=[]\n",
    "data_dict['IS 664']=[]\n",
    "data_dict['IS 676']=['IS 663','CS 673']\n",
    "data_dict['IS 677']=[]\n",
    "data_dict['IS 682']=[]\n",
    "data_dict['IS 683']=[]\n",
    "data_dict['IS 686']=[]\n",
    "data_dict['IS 698']=[]\n",
    "data_dict['IS 700']=[]\n",
    "data_dict['IS 700B']=[]\n",
    "data_dict['IS 700C']=[]\n",
    "data_dict['IS 701']=[]\n",
    "data_dict['IS 701B']=[]\n",
    "data_dict['IS 701C']=[]\n",
    "data_dict['IS 725']=[]\n",
    "data_dict['IS 764']=[]\n",
    "data_dict['IS 766']=[]\n",
    "data_dict['IS 776']=[]\n",
    "data_dict['IS 785']=[]\n",
    "data_dict['IS 786']=[]\n",
    "data_dict['IS 790']=[]\n",
    "data_dict['IS 790D']=[]\n",
    "data_dict['IS 790E']=[]\n",
    "data_dict['IS 790F']=[]\n",
    "data_dict['IS 791']=[]\n",
    "data_dict['IT 610']=[]\n",
    "data_dict['IT 635']=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 853,
     "status": "ok",
     "timestamp": 1601395859909,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "in4xSiiRwFkR"
   },
   "outputs": [],
   "source": [
    "#removing the nan value\n",
    "del data_dict['nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1238,
     "status": "ok",
     "timestamp": 1601395862778,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "5Bw9ODlLwJQp"
   },
   "outputs": [],
   "source": [
    "#loading  University-1 graduate courses csv file\n",
    "Univ1_graduate_dataset=pd.read_csv('Univ1_graduate_courses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 394,
     "status": "ok",
     "timestamp": 1601395863960,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "qMQm-K8NwLg8"
   },
   "outputs": [],
   "source": [
    "#removing unicode characters from course\n",
    "Univ1_graduate_dataset['course']=Univ1_graduate_dataset['course'].str.replace('¬†',' ')\n",
    "Univ1_graduate_dataset.drop('Unnamed: 3',axis=1,inplace=True)\n",
    "\n",
    "#cleansing of data\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "#Genrating the frequent words\n",
    "def gen_freq(text):\n",
    "    #to store the list of words\n",
    "    word_list = []\n",
    "    for tw_words in text:\n",
    "        word_list.extend(tw_words)\n",
    "    #Create word frequencies using word_list\n",
    "    word_freq = pd.Series(word_list).value_counts()\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 972,
     "status": "ok",
     "timestamp": 1601395866892,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "8TTmqXDDwmLd"
   },
   "outputs": [],
   "source": [
    "#core courses list for ms in cyberscecurity\n",
    "cyber_core_list=['CS 608','CS 645','CS 646','CS 647','CS 656','CS 696']\n",
    "#elective courses list for ms in cybersecurity\n",
    "cyber_elective_list=['CS 633','CS 634','CS 643','CS 648','CS 652','CS 660','CS 673','CS 678','CS 680','CS 684','CS 708','CS 755','IS 680','IS 681','IS 682','IS 687','IT 620','IT 640','ECE 636','MGMT 688','MGMT 691','CS 610','CS 630','CS 631']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1601395867967,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "pZQew8zFwmFR"
   },
   "outputs": [],
   "source": [
    "#complete courses list for cybersecurity\n",
    "cyber_courses_list=cyber_core_list + cyber_elective_list\n",
    "len(cyber_courses_list)\n",
    "\n",
    "#to compile one dictionary  for core and elective course where 1 will denote course is core and 0 will denote elective\n",
    "cyber_dict={}\n",
    "for i in range(len(cyber_courses_list)):\n",
    "  key=cyber_courses_list[i]\n",
    "  if key in cyber_core_list:\n",
    "    cyber_dict[key]=[1,0]\n",
    "  else:\n",
    "    cyber_dict[key]=[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1095,
     "status": "ok",
     "timestamp": 1601395871044,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "hK6fnrFww3eb"
   },
   "outputs": [],
   "source": [
    "#creating dictionary for cybersecurity courses and prerequsite\n",
    "def prereq_dict(data_dict,course_list):\n",
    "  course_dict={}\n",
    "  for i in range(len(course_list)):\n",
    "    key=course_list[i]\n",
    "    if key in data_dict:\n",
    "      course_dict[key]=data_dict.get(key)\n",
    "    else:\n",
    "      course_dict[key]=[]\n",
    "  return course_dict\n",
    "\n",
    "#cybersecurity  dict with prerequisite information\n",
    "cyber_prereq_dict=prereq_dict(data_dict,cyber_courses_list)\n",
    "\n",
    "#Adding double brackets for courses with prereqs in 'and' condition and correcting other courses prereqs\n",
    "cyber_prereq_dict['ECE 636']=['CS 656']\n",
    "cyber_prereq_dict['CS 643']=['CS 656','CS 644']\n",
    "cyber_prereq_dict['CS 646']=['CS 656']\n",
    "cyber_prereq_dict['CS 647']=['CS 645','CS 646','CS 696']\n",
    "cyber_prereq_dict['CS 708']=[['CS 608', 'CS 645', 'CS 696']]\n",
    "cyber_prereq_dict['IT 620']=[]\n",
    "cyber_prereq_dict['CS 648']=['CS 656']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 978,
     "status": "ok",
     "timestamp": 1601395873941,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "Ew3FPGiexBNf"
   },
   "outputs": [],
   "source": [
    "#compare the dataset with cyber courses list and extract them\n",
    "cyber_df=Univ1_graduate_dataset.loc[Univ1_graduate_dataset['course'].isin(cyber_courses_list)]\n",
    "\n",
    "#Add  the 3 missing courses from  cyber courses list in the cyber dataframe\n",
    "listOfSeries = [pd.Series(['ECE 636','Computer Networking Laboratory','Prerequisites: ECE 637 or CS 656. This course provides students with hands on training regarding the design, troubleshooting, modeling and evaluation of computer networks. In this course, students are going to experiment in a real test-bed networking environment, and learn about network design and troubleshooting topics and tools such as: network addressing, Address Resolution Protocol (ARP), basic troubleshooting tools (e.g. ping, ICMP), IP routing (e,g, RIP), route discovery (e.g. traceroute), TCP and UDP, IP fragmentation and many others. Student will also be introduced to the network modeling and simulation, and they will have the opportunity to build some simple networking models using the OPNET modeling tool and perform simulations that will help them evaluate their design approaches and expected network performance.'], index=cyber_df.columns ) ,\n",
    "                pd.Series(['MGMT 688','Information Technology Business and the law','Includes historical and constitutional foundations, crimes, and torts in cyberspace, virtual property (patents online, copyrights in digital information, trade secrets in cyberspace, and cybermarks), electronic commerce contracting, electronic commerce, electronic money and the law, and information technology and online infringement of rights of intellectual property.'], index=cyber_df.columns ) ,\n",
    "                pd.Series(['MGMT 691','Legal and Ethical issues','Explores the legal and ethical responsibilities of managers. Analyzes extent to which shareholders should be allowed to exercise their legitimate economic, legal, and ethical claims on corporate managers; extent of regulation of a particular industry, individual rights of the employee and various corporate interests, and corporate responsibility to consumers, society, and conservation of natural resources and the environment.'], index=cyber_df.columns ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 369,
     "status": "ok",
     "timestamp": 1601395875246,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "GidBn7JaxIVF"
   },
   "outputs": [],
   "source": [
    "# Pass a list of series to the append() to add multiple rows\n",
    "cyber_df = cyber_df.append(listOfSeries , ignore_index=True)\n",
    "\n",
    "#to reset the index \n",
    "cyber_df=cyber_df.reset_index(drop=True)\n",
    "\n",
    "#converting the course_description and course_name column data into string format\n",
    "cyber_df['course description']=cyber_df['course description'].apply(str)\n",
    "cyber_df['course_name']=cyber_df['course_name'].apply(str)\n",
    "\n",
    "#applying the clean_text function to the course_name column\n",
    "cyber_df['course_name']=cyber_df['course_name'].apply(clean_text)\n",
    "\n",
    "\n",
    "#tokenizing the words from the text in the course_name column\n",
    "def getting_nouns(text):\n",
    "  text_blob_object=TextBlob(text)\n",
    "\n",
    "  return text_blob_object.words\n",
    "\n",
    "\n",
    "cyber_df['words']=cyber_df['course_name'].apply(getting_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "executionInfo": {
     "elapsed": 826,
     "status": "ok",
     "timestamp": 1601395878179,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "tdAreSsyxant",
    "outputId": "2caa2970-6c30-4a14-956a-ca157659ae4d"
   },
   "outputs": [],
   "source": [
    "#removing stopwords using nltk library\n",
    "cyber_df['words'] = cyber_df['words'].apply(lambda x:[item for item in x if item not in stop])\n",
    "\n",
    "#converting the words column into a single list \n",
    "list_of_topics=cyber_df['words'].tolist()\n",
    "\n",
    "#making a single list\n",
    "merged=list(itertools.chain.from_iterable(list_of_topics))\n",
    "\n",
    "#removing the duplicates from the list\n",
    "cyber_topics=list(dict.fromkeys(merged))\n",
    "print(*cyber_topics,sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 870,
     "status": "ok",
     "timestamp": 1601395880659,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "EAOxw0Kfyo0o",
    "outputId": "941e16cb-d2eb-448a-e01b-98bd9dbb6c9d"
   },
   "outputs": [],
   "source": [
    "cyber_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1601395881713,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "Cgnn7wZzyikS",
    "outputId": "509ad287-605a-4e85-f7a8-0e4d21bf8171"
   },
   "outputs": [],
   "source": [
    "len(cyber_courses_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 3995,
     "status": "ok",
     "timestamp": 1601395887489,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "WfOvsQrExqGP",
    "outputId": "b70d6ba1-f0db-4d1c-872e-f9330a375638"
   },
   "outputs": [],
   "source": [
    "edges=[]\n",
    "for course in cyber_courses_list:\n",
    "  #print(course)\n",
    "  if (cyber_prereq_dict[course]==[]):\n",
    "    CourseId=cyber_courses_list[:]\n",
    "    CourseId.remove(course)\n",
    "    #print(CourseId)\n",
    "    for i in CourseId:\n",
    "      # print(course , (i,course))\n",
    "      edges.append((i,course))\n",
    "  else:\n",
    "      if (len(cyber_prereq_dict[course])!=1 or type(cyber_prereq_dict[course][0])==str):\n",
    "        for j in cyber_prereq_dict[course]:\n",
    "          if(j== [] or j==course):\n",
    "            continue\n",
    "          else:\n",
    "            # print(course , (j,course))\n",
    "            edges.append((j, course)) \n",
    "      else:\n",
    "        l = list(permutations(cyber_prereq_dict[course][0])) \n",
    "        for k in l:\n",
    "          m=0\n",
    "          while (m in range(len(k)-1)):\n",
    "            if ((k[m],k[m+1]) in edges)==False:\n",
    "              edges.append((k[m],k[m+1]))\n",
    "            m+=1\n",
    "          if ((k[len(k)-1],course) in edges)==False:   \n",
    "            edges.append((k[len(k)-1],course))\n",
    "\n",
    "\n",
    "\n",
    "for course in cyber_courses_list:\n",
    "  if(cyber_prereq_dict[course]==[]):\n",
    "    edges.append(('initial' , course))\n",
    "# print(CourseId)\n",
    "# print(edges)\n",
    "       \n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(edges) \n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw_networkx_nodes(G,pos, cmap=plt.get_cmap('jet'),node_size = 1000,node_color=\"lightblue\")\n",
    "nx.draw_networkx_edges(G,pos,edge_color='b', edge_cmap=plt.cm.Blues,arrows=True,arrowstyle=\"->\",arrowsize=10)\n",
    "nx.draw_networkx_labels(G,pos)\n",
    "\n",
    "plt.show()\n",
    "#pylab.show()\n",
    "print(len(edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1633,
     "status": "ok",
     "timestamp": 1601395900047,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "EFA4-VHcyWeY"
   },
   "outputs": [],
   "source": [
    "#creating a movie id dictionary with key as course and node number as value\n",
    "movies = list(G.nodes())\n",
    "movie_ids={}\n",
    "for i,node in enumerate(G.nodes()):\n",
    "  movie_ids[node]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1493,
     "status": "ok",
     "timestamp": 1601395902261,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "mv2fJyMQyXtx"
   },
   "outputs": [],
   "source": [
    "#creating the topic dictonary with key as course name and topic\n",
    "topic_dict={}\n",
    "course_topics=list(cyber_df['words'])\n",
    "for i in range(len(cyber_courses_list)):\n",
    "  topic_dict[cyber_courses_list[i]]=course_topics[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "executionInfo": {
     "elapsed": 890,
     "status": "ok",
     "timestamp": 1601395904084,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "4Ty_HVMuywx4",
    "outputId": "56cc1c29-ff06-4ac9-9d56-1401bf86a9b4"
   },
   "outputs": [],
   "source": [
    "#printing the topic_dict \n",
    "for i,j in topic_dict.items():\n",
    "  print('{} : {}'.format(i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 844,
     "status": "ok",
     "timestamp": 1601395907909,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "KCGaEMwTM0Ch"
   },
   "outputs": [],
   "source": [
    "#Creating the input sequence for omega\n",
    "a=random.sample(cyber_core_list,6)\n",
    "b=random.sample(cyber_elective_list,4)\n",
    "#creating the shuffle function\n",
    "def shuffle(a,b):\n",
    "  list_of_sequences=[]\n",
    "  sequence_list=[]\n",
    "  i=0\n",
    "  while i<1000:\n",
    "    sequence_list=[i] + list(a + b)\n",
    "    list_of_sequences.append(sequence_list)\n",
    "    #to randomly select the 6 core courses from the list\n",
    "    a=random.sample(a,6)\n",
    "    #to randomly the select 4 elective courses from the list\n",
    "    b=random.sample(cyber_elective_list,4)\n",
    "    sequence_list=[]\n",
    "    i=i+1\n",
    "  return list_of_sequences\n",
    "\n",
    "list_sequences=shuffle(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 19365,
     "status": "ok",
     "timestamp": 1601395933137,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "B1ztaQ9nyz9C",
    "outputId": "eb113d01-e632-4fa4-b763-5676b9d96626"
   },
   "outputs": [],
   "source": [
    "# split the sequence of a user into two sequences one to add to the graph and the other to use it as ground truth for evaluation\n",
    "def divide_test(test_user_sequence):\n",
    "    user_id = test_user_sequence[0]\n",
    "    items_in_sequence = test_user_sequence[1:]\n",
    "    l = len(items_in_sequence)//2\n",
    "    previous = items_in_sequence[:l]\n",
    "\n",
    "    left = items_in_sequence[l:]\n",
    "\n",
    "    return user_id , previous , left\n",
    "\n",
    "def compute_edges(gra, sequence):\n",
    "    #start_time = time.time()\n",
    "    SG= nx.DiGraph()\n",
    "    SG.add_nodes_from(sequence)\n",
    "\n",
    "    x=gra.subgraph(sequence)\n",
    "\n",
    "    for i in range(0,len(sequence)):\n",
    "        for p in range(i, len(sequence)):\n",
    "            if (sequence[i],sequence[p]) in x.edges():\n",
    "                SG.add_edge(sequence[i],sequence[p], weight= x[sequence[i]][sequence[p]]['weight'])\n",
    "    #print(' compute edges seconds', time.time() - start_time)\n",
    "    return SG\n",
    "    \n",
    "\n",
    "def somme(arcs):\n",
    "    l = list(arcs.edges())\n",
    "    sum=0\n",
    "    for tup in l:\n",
    "        sum=sum + arcs[tup[0]][tup[1]]['weight']\n",
    "    return sum\n",
    "\n",
    "def utility_sum(gra,sequence):\n",
    "    return (somme(compute_edges(gra,sequence)))\n",
    "\n",
    "# For the conditional variant of OMEGA\n",
    "def noeuds(E,S):   # E is a set of tuples (l), S is a list of items (nodes)\n",
    "\n",
    "    a, b = zip(*E)\n",
    "    S =[ i for i in S ]\n",
    "    l = list(a)+list(b)\n",
    "    y= list(set(l))\n",
    "    z= list(set(y)-set(S))\n",
    "\n",
    "    return z\n",
    "\n",
    "\n",
    "def noeuds1(E):\n",
    "    a,b = zip(*E)\n",
    "    l = list(a)+list(b)\n",
    "    y= list(set(l))\n",
    "    return y\n",
    "\n",
    "def Union(E,e):\n",
    "    return E.union(e)\n",
    "\n",
    "def Union_List(lst1, lst2): \n",
    "    final_list = list(set(lst1) | set(lst2)) \n",
    "    return len(final_list)\n",
    "\n",
    "###### REORDER\n",
    "\n",
    "def REORDER(gra,structure,E,S):  # Compute sequence of items from a set of  edges E according to a graph gra\n",
    "    #np.random.seed(123)  # to have for every call of the function the same topological structure\n",
    "    # works in  k.log(k) |A|=k\n",
    "\n",
    "    A = noeuds(E,S)\n",
    "    #start_time = time.time()\n",
    "    b= sorted(A, key=lambda x: structure.index(x))\n",
    "    #print(' REORDER seconds', time.time() - start_time)\n",
    "\n",
    "    return b\n",
    "\n",
    "\n",
    "def REORDER1(gra,structure,E):  # Compute sequence of items from a set of  edges E according to a graph gra\n",
    "    #np.random.seed(123)  # to have for every call of the function the same topological structure\n",
    "    # works in  k.log(k) |A|=k\n",
    "\n",
    "    A = noeuds1(E)\n",
    "    #start_time = time.time()\n",
    "    b= sorted(A, key=lambda x: structure.index(x))\n",
    "    #print(' REORDER seconds', time.time() - start_time)\n",
    "\n",
    "    return b\n",
    "\n",
    "# OMEGA\n",
    "def OMEGA(gra,structure,  k,S):\n",
    "\n",
    "    edges = set()\n",
    "    arcs = set(gra.edges()) # set of (edges) tuples [ (0,1) , (1,2),....   ]\n",
    "    #print(len(arcs))\n",
    "    # dif = [x for x in arcs if x not in edges]\n",
    "    dif = arcs-edges\n",
    "    C=[]    #  implementing the set C and while condition\n",
    "    for e in dif:\n",
    "        #print(Union( edges,[e]))\n",
    "\n",
    "        if len(noeuds(Union( edges,[e]) ,S)) <= k:\n",
    "            C.append(e)\n",
    "    #utilities.append (utility(gra, REORDER(gra,Union(edges,[e]))))\n",
    "\n",
    "\n",
    "    while C :\n",
    "        maximum = -1   #searching in C the edge that will maximizes the utility function\n",
    "        for i in C:\n",
    "            value  = utility_sum(gra, REORDER1(gra,structure, Union(edges,[i]) ) )\n",
    "            #print('utility', value)\n",
    "            if maximum < value:\n",
    "                maximum= value\n",
    "                tup= i # the edge to add with the maximum utility\n",
    "        \n",
    "        edges.add(tup)\n",
    "\n",
    "#print(edges)\n",
    "        if len(noeuds(edges,S )) ==k:\n",
    "                return  REORDER( gra,structure, edges,S  )\n",
    "\n",
    "        dif = arcs-edges\n",
    "       \n",
    "   #implementing the set C and while condition\n",
    "        C=set()\n",
    "        \n",
    "\n",
    "        for e in dif:\n",
    "\n",
    "            if len(noeuds(Union( edges,[e]),S )) <= k:\n",
    "               C.add(e)\n",
    "\n",
    "#print(C)\n",
    "    return  REORDER( gra,structure, edges,S  )\n",
    "\n",
    "\n",
    "def Precision_k(gra,structure, test_user_sequence, k):\n",
    "    \n",
    "    user_id, previous, left = divide_test(test_user_sequence)\n",
    "    reco = OMEGA(gra,structure,k, previous)\n",
    "    reco =[str(i) for i in reco]\n",
    "    \n",
    "    print(reco)\n",
    "    intersection = list(set(reco) & set(left) )\n",
    "    return (len (intersection)/k)\n",
    "\n",
    "\n",
    "\n",
    "# Our Graph Creation for a sequence (  one user  )\n",
    "def Create_OUR(test_user_sequence,z):\n",
    "\n",
    "    transitions = pk.load(open('final_matrix2', 'rb'))\n",
    "    user_id, previous, left = divide_test(test_user_sequence)\n",
    "    candidate_items = list(set(movies).difference(previous))\n",
    "    #candidate_items = left\n",
    "    gra = nx.DiGraph()\n",
    "    for j in previous:\n",
    "        gra.add_node(j)\n",
    "\n",
    "    for i in candidate_items:\n",
    "\n",
    "        gra.add_node(i)\n",
    "        gra.add_edge(i,i)\n",
    "        index_i = movie_ids[i]\n",
    "        gra[i][i]['weight'] = transitions[index_i, index_i]\n",
    "\n",
    "    last_items = previous[-z:]\n",
    "    for item in last_items:\n",
    "        index_last = movie_ids[item]\n",
    "\n",
    "        for i in candidate_items:\n",
    "            index_item = movie_ids[i]\n",
    "            gra.add_edge(item,i)\n",
    "            gra[item][i]['weight'] = transitions[ index_last,index_item]\n",
    "\n",
    "    return gra\n",
    "\n",
    "\n",
    "def worker( argument, results):\n",
    "    \n",
    "    G= nx.DiGraph()\n",
    "\n",
    "    G = Create_OUR(argument,3)\n",
    "\n",
    "    #print(G.edges(data=True))\n",
    "\n",
    "    toto = nx.DiGraph()\n",
    "\n",
    "    toto = G.copy()\n",
    "\n",
    "    toto.remove_edges_from(nx.selfloop_edges(toto,data=True))  # toto\n",
    "\n",
    "    structure = list(nx.topological_sort(toto))  # toto\n",
    "    \n",
    "     \n",
    "    results.append( Precision_k( G,structure,argument,5))\n",
    "\n",
    "\n",
    "random.seed(123)\n",
    "# In the paper, they used 500 sequences at random as a test set\n",
    "# Split sequences into training and testing\n",
    "\n",
    "def make_train_test (sequences, size):\n",
    "\n",
    "    list_shuffled = list_sequences.copy()\n",
    "    #random.shuffle(list_shuffled)\n",
    "    train_data = list_shuffled[size:]\n",
    "    test_data =list_shuffled[:size]\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "##########################\n",
    "\n",
    "def proba(sequences, list_items,topic_dict):\n",
    "    number_of_items = len(list_items)\n",
    "    matrice = np.zeros(shape=(number_of_items,number_of_items))  # to store number of occurences of topics with respect to the threshold l\n",
    "    matrice1 = np.zeros(shape=(number_of_items,number_of_items)) # to store number of occurences\n",
    "\n",
    "    # Version of the probas without the parameter l\n",
    "    for seq in sequences:\n",
    "         items = seq[1:]\n",
    "         for i in range(len(items)-1):\n",
    "            for j in range(i+1, len(items)):\n",
    "                \n",
    "                index_i = list_items[items[i]]\n",
    "                \n",
    "                index_j = list_items[items[j]]\n",
    "                matrice[index_i][index_j] += Union_List(topic_dict[items[i]],topic_dict[items[j]])\n",
    "    \n",
    "    return matrice\n",
    "\n",
    "#Counting the occurance of an item in all the lists we have\n",
    "def frequency (train, list_items,topic_dict):\n",
    "    number_of_items = len(list_items)\n",
    "    list_count = [0]* number_of_items\n",
    "\n",
    "    for sequence in train:\n",
    "        items = sequence[1:]\n",
    "        for item in items:\n",
    "          topic=0\n",
    "          if item in list_items:\n",
    "              index_item = list_items.index(item)\n",
    "              topic=len(topic_dict[item]) \n",
    "              list_count[index_item]+=topic\n",
    "\n",
    "    for i in range(len(list_count)):\n",
    "        if list_count[i]<10:\n",
    "            list_count[i]=0\n",
    "\n",
    "    return list_count\n",
    "\n",
    "# Dividing the occurance of each item by the number of lists we have in our set of sequences\n",
    "def empirical_frequency_items(train, list_items,topic_dict):  # train: training set of sequences , items list of item ids\n",
    "\n",
    "    number_of_items = len(list_items)\n",
    "    list_count = [0]* number_of_items\n",
    "\n",
    "    for sequence in train:\n",
    "        items = sequence[1:]\n",
    "        for item in items:\n",
    "          topic=0\n",
    "          if item in list_items:\n",
    "            index_item = list_items.index(item)\n",
    "            topic=len(topic_dict[item]) \n",
    "            list_count[index_item]+=1\n",
    "\n",
    "    for i in range(len(list_count)):\n",
    "        if list_count[i]<10:\n",
    "            list_count[i]=0\n",
    "\n",
    "    return [x/len(train) for x in list_count]\n",
    "\n",
    "\n",
    "def Transition_matrix (list_sequences, movies):\n",
    "\n",
    "    matrice= pk.load(open ('proba_matrix_without_l', 'rb'))\n",
    "    # first we run Proba function to get the proba_matrix_without_l, we store it and loaod it for faster tests\n",
    "\n",
    "    final_matrix = np.zeros(shape= (len(movies), len(movies)))\n",
    "\n",
    "    frequencies = frequency(list_sequences, movies,topic_dict)\n",
    "    emp_freq = empirical_frequency_items(list_sequences, movies,topic_dict)\n",
    "    for i in range(len(movies)):\n",
    "        for j in range(len(movies)):\n",
    "            if i == j:\n",
    "                final_matrix[i,i]= emp_freq[i]\n",
    "                \n",
    "            else :\n",
    "                if frequencies[i] ==0:\n",
    "                    final_matrix[i,j]= 0\n",
    "                else:\n",
    "                    final_matrix[i,j]= matrice[i,j]/frequencies[i]\n",
    "    return final_matrix\n",
    "\n",
    "\n",
    "#####################\n",
    "\n",
    "#dividing the data into test and train\n",
    "\n",
    "train_data, test_data = make_train_test(list_sequences,150)\n",
    "\n",
    "m = proba(train_data, movie_ids,topic_dict)\n",
    "print('part2')\n",
    "pk.dump(m, open('proba_matrix_without_l','wb'))\n",
    "\n",
    "print('start')\n",
    "final_matrix = Transition_matrix(train_data, movies)\n",
    "pk.dump(final_matrix , open('final_matrix2', 'wb')) \n",
    "\n",
    "start_time = time.time()\n",
    "manager = multiprocessing.Manager()\n",
    "results = manager.list()\n",
    "\n",
    "jobs=[]\n",
    "for i in range(150):\n",
    "        p = multiprocessing.Process(target = worker, args=(test_data[i], results))\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "\n",
    "for proc in jobs:\n",
    "    proc.join()\n",
    "print ('sum(results)/len(results)',sum(results)/len(results))\n",
    "print('whole', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32YbEzZxNv6w"
   },
   "outputs": [],
   "source": [
    "#save the results from omega framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hARXQCwkzbbZ"
   },
   "outputs": [],
   "source": [
    "list_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AdDR-B4M1q2_"
   },
   "outputs": [],
   "source": [
    "#load the results obtained from the omega\n",
    "data=open('omega_results_cyber.txt','r')\n",
    "\n",
    "data_2=data.readlines()\n",
    "results_sequences=[]\n",
    "for i in range(150):\n",
    "  line=data_2[i]\n",
    "  line_1=ast.literal_eval(line)\n",
    "  line_2=[i] + line_1\n",
    "  line_3=line_2\n",
    "  results_sequences.append(line_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BiXpeNCg7jMY"
   },
   "outputs": [],
   "source": [
    "data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 455,
     "status": "ok",
     "timestamp": 1599974847417,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "f2YHcoXF2hh3",
    "outputId": "203ddf1d-8831-41e0-96cc-aa73182f9abd"
   },
   "outputs": [],
   "source": [
    "len(results_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVU0Rgtb2lH0"
   },
   "outputs": [],
   "source": [
    "#taking the corresponding sequence for the test results to combine\n",
    "test_sequence=list_sequences[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gSrQGeo2uUR"
   },
   "outputs": [],
   "source": [
    "test_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "743cuZYwgmts"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OU6KM79p2prz"
   },
   "outputs": [],
   "source": [
    "#creating final results sequence with previous sequence from the input sequence with sequence results obtained from the omega framework\n",
    "final_sequence=[]\n",
    "for i in range(len(test_sequence)):\n",
    "  list_1=test_sequence[i]\n",
    "  list_2=list_1[1:6]\n",
    "  #print(list_2)\n",
    "  list_3=results_sequences[i]\n",
    "  list_4=list_3[1:]\n",
    "  #print(list_4)\n",
    "  final_sequence.append(list_2 + list_4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "executionInfo": {
     "elapsed": 530,
     "status": "ok",
     "timestamp": 1599974878654,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "JgBeIfPY20p0",
    "outputId": "42b706ea-1f05-4e0e-c775-9859aeddfb04"
   },
   "outputs": [],
   "source": [
    "print(*final_sequence,sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7m54m_f3OhU"
   },
   "outputs": [],
   "source": [
    "#saving the final resulting sequences\n",
    "with open('test_results_cyber.txt', 'w') as f:\n",
    "    for item in final_sequence:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOTehr1rgtGOEf+iU9H97sd",
   "mount_file_id": "1sek4LOr6D-WOfBaSmq7ezTJNCTw-UQoB",
   "name": "Omega_Cyber.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}