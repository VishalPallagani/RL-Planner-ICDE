{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 3890,
     "status": "ok",
     "timestamp": 1601395967526,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "rkxBMNK1tz9B",
    "outputId": "9c36090a-adce-47fe-9acf-7c861981914c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random \n",
    "import re\n",
    "from scipy import spatial\n",
    "%matplotlib inline\n",
    "import math \n",
    "import ast \n",
    "from collections import Counter \n",
    "import nltk\n",
    "from operator import itemgetter \n",
    "nltk.download('wordnet')\n",
    "from scipy.spatial import distance\n",
    "import sys\n",
    "import itertools\n",
    "from itertools import permutations \n",
    "import pylab\n",
    "from textblob import TextBlob\n",
    "import itertools as it\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle as pk\n",
    "import multiprocessing\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 808,
     "status": "ok",
     "timestamp": 1601395996524,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "M42v_XfPucH6"
   },
   "outputs": [],
   "source": [
    "#Univ 1 graduate courses list\n",
    "Univ1_dataset=pd.read_csv('Univ-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 897,
     "status": "ok",
     "timestamp": 1601395998521,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "7EMXzsdmuqjf"
   },
   "outputs": [],
   "source": [
    "Univ1_dataset.head()\n",
    "Univ1_dataset.dtypes\n",
    "Univ1_dataset=Univ1_dataset.applymap(str)\n",
    "#removing the non-ascii characters present in the course and prerequisite column on university dataset\n",
    "Univ1_dataset['Course']=Univ1_dataset['Course'].str.replace('\\xa0',' ')\n",
    "Univ1_dataset['Prerequisite']=Univ1_dataset['Prerequisite'].str.replace('\\xa0',' ')\n",
    "\n",
    "#removing the quotes and backslash from the course column of the dataset\n",
    "def quotes(text):\n",
    "  text = text.strip('\\\"')\n",
    "  text=text.strip()\n",
    "  return text\n",
    "Univ1_dataset['Course']=Univ1_dataset['Course'].apply(quotes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 986,
     "status": "ok",
     "timestamp": 1601396001126,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "OR0_qPJQu1N7"
   },
   "outputs": [],
   "source": [
    "#tokenizing the prerquisites\n",
    "\n",
    "Univ1_dataset['Prerequisite'] = Univ1_dataset['Prerequisite'].str.split(',')\n",
    "#removing extra spaces in the pre list\n",
    "Univ1_dataset['Prerequisite'] = Univ1_dataset['Prerequisite'].apply(lambda x:[w.lstrip() for w in x])\n",
    "Univ1_dataset[Univ1_dataset['Course']=='CS 643'].values\n",
    "course=list(Univ1_dataset['Course'])\n",
    "pre=list(Univ1_dataset['Prerequisite'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 880,
     "status": "ok",
     "timestamp": 1601396002879,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "vZ257Vw6u_yg"
   },
   "outputs": [],
   "source": [
    "#removing the prereq which are not university-1 graduate courses\n",
    "\n",
    "for i in range(len(pre)):\n",
    "  list_1=pre[i]\n",
    "  for index,j in enumerate(list_1):\n",
    "    if j not in course:\n",
    "      list_1[index]='nan'\n",
    "\n",
    "#removing the 'nan' values\n",
    "for i in range(len(pre)):\n",
    "  list_1=pre[i]\n",
    "  for index,j in enumerate(list_1):\n",
    "    if j=='nan':\n",
    "      list_1.remove('nan')\n",
    "for i in range(len(pre)):\n",
    "  list_1=pre[i]\n",
    "  for index,j in enumerate(list_1):\n",
    "    if j=='nan':\n",
    "      list_1.remove('nan')\n",
    "for i in range(len(pre)):\n",
    "  list_1=pre[i]\n",
    "  for index,j in enumerate(list_1):\n",
    "    if j=='nan':\n",
    "      list_1.remove('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1121,
     "status": "ok",
     "timestamp": 1601396005699,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "HlfbRY8avLlO"
   },
   "outputs": [],
   "source": [
    "#data dict represents key as course from University-1 graduate courses and its prerequisite as value\n",
    "data_dict={}\n",
    "for i in range(len(course)):\n",
    "  data_dict[course[i]]=pre[i]\n",
    "#removing redundancies in the data dictionary\n",
    "data_dict['BNFO 700B']=[]\n",
    "data_dict['BNFO 701B']=[]\n",
    "data_dict['IS 701B']=[]\n",
    "data_dict['CS 735']=['CS 675', 'CS 634', 'CS 643', 'CS 644']\n",
    "data_dict['CS 726']=[]\n",
    "data_dict['CS 700B']=[]\n",
    "data_dict['CS 635']=['CS 505','CS 510']\n",
    "data_dict['CS 640']=['CS 540']\n",
    "data_dict['CS 646']=['CS 645','CS 646','CS 696']\n",
    "data_dict['CS 650']=['CS 510']\n",
    "data_dict['CS 668']=['CS 610','CS 650']\n",
    "data_dict['CS 696']=['CS 652','CS 656']\n",
    "data_dict['CS 701C']=[]\n",
    "data_dict['CS 725']=[]\n",
    "data_dict['CS 790A']=['CS 791']\n",
    "data_dict['CS 790B']=['CS 791']\n",
    "data_dict['CS 790C']=['CS 791']\n",
    "data_dict['IS 591']=[]\n",
    "data_dict['IS 592']=[]\n",
    "data_dict['IS 593']=[]\n",
    "data_dict['IS 601']=[]\n",
    "data_dict['IS 612']=[]\n",
    "data_dict['IS 613']=[]\n",
    "data_dict['IS 614']=[]\n",
    "data_dict['IS 616']=[]\n",
    "data_dict['IS 650']=[]\n",
    "data_dict['IS 661']=[]\n",
    "data_dict['IS 664']=[]\n",
    "data_dict['IS 676']=['IS 663','CS 673']\n",
    "data_dict['IS 677']=[]\n",
    "data_dict['IS 682']=[]\n",
    "data_dict['IS 683']=[]\n",
    "data_dict['IS 686']=[]\n",
    "data_dict['IS 698']=[]\n",
    "data_dict['IS 700']=[]\n",
    "data_dict['IS 700B']=[]\n",
    "data_dict['IS 700C']=[]\n",
    "data_dict['IS 701']=[]\n",
    "data_dict['IS 701B']=[]\n",
    "data_dict['IS 701C']=[]\n",
    "data_dict['IS 725']=[]\n",
    "data_dict['IS 764']=[]\n",
    "data_dict['IS 766']=[]\n",
    "data_dict['IS 776']=[]\n",
    "data_dict['IS 785']=[]\n",
    "data_dict['IS 786']=[]\n",
    "data_dict['IS 790']=[]\n",
    "data_dict['IS 790D']=[]\n",
    "data_dict['IS 790E']=[]\n",
    "data_dict['IS 790F']=[]\n",
    "data_dict['IS 791']=[]\n",
    "data_dict['IT 610']=[]\n",
    "data_dict['IT 635']=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 823,
     "status": "ok",
     "timestamp": 1601396008826,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "u5OD7VJcvT_r"
   },
   "outputs": [],
   "source": [
    "#removing the nan value\n",
    "del data_dict['nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 847,
     "status": "ok",
     "timestamp": 1601396010504,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "iQOIWD82valH",
    "outputId": "c33957b6-ccbe-4406-c140-d22f1c17426e"
   },
   "outputs": [],
   "source": [
    "#data science core courses for computational track\n",
    "data_science_core_list=['CS 675','CS 644','CS 636','CS 677','MATH 661']\n",
    "#data science elective courses related to computer science field\n",
    "data_science_elective_list=['CS 610','CS 631','CS 632','CS 634','CS 639','CS 643','CS 645','CS 656','CS 659','CS 661','CS 670','CS 676','CS 683','CS 684','CS 681','CS 708','CS 731','CS 732','CS 735','CS 744','CS 782','CS 608','CS 633','CS 505','CS 696','CS 652']\n",
    "#combining the core and elective courses to create one list\n",
    "data_science_courses_list=data_science_core_list + data_science_elective_list\n",
    "len(data_science_courses_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 847,
     "status": "ok",
     "timestamp": 1601396013033,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "qc-6ZtDJvm4u"
   },
   "outputs": [],
   "source": [
    "#to compile one dictinary core and elective course where 1 will denote course is core and 0 will denote elective\n",
    "data_science_dict={}\n",
    "for i in range(len(data_science_courses_list)):\n",
    "  key=data_science_courses_list[i]\n",
    "  if key in data_science_core_list:\n",
    "    data_science_dict[key]=[1,0]\n",
    "  else:\n",
    "    data_science_dict[key]=[0,1]\n",
    "#creating dictionary for data science courses and prerequsite\n",
    "def prereq_dict(data_dict,course_list):\n",
    "  course_dict={}\n",
    "  for i in range(len(course_list)):\n",
    "    key=course_list[i]\n",
    "    if key in data_dict:\n",
    "      course_dict[key]=data_dict.get(key)\n",
    "    else:\n",
    "      course_dict[key]=[]\n",
    "  return course_dict\n",
    "\n",
    "  \n",
    "data_science_prereq_dict=prereq_dict(data_dict,data_science_courses_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 975,
     "status": "ok",
     "timestamp": 1601396015864,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "Mn_WDyvCvwjh"
   },
   "outputs": [],
   "source": [
    "#making double brackets for prereqs with and condition or single course\n",
    "data_science_prereq_dict['CS 708']=[['CS 608','CS 645','CS 696']]\n",
    "data_science_prereq_dict['CS 735']=[['CS 675','CS 634','CS 631','CS 644']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1073,
     "status": "ok",
     "timestamp": 1601396020662,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "wbTALN4Wv0vu"
   },
   "outputs": [],
   "source": [
    "#load Univ 1 graduate courses csv file\n",
    "Univ1_graduate_dataset=pd.read_csv('Univ1_graduate_courses.csv')\n",
    "Univ1_graduate_dataset.head()\n",
    "Univ1_graduate_dataset[Univ1_graduate_dataset['course']=='BNFO 601'].values\n",
    "#removing unicode characters from course\n",
    "Univ1_graduate_dataset['course']=Univ1_graduate_dataset['course'].str.replace('¬†',' ')\n",
    "#removing the Unnamed: 3 column from the dataset\n",
    "Univ1_graduate_dataset.drop('Unnamed: 3',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1038,
     "status": "ok",
     "timestamp": 1601396022659,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "ql-a0pvawATs"
   },
   "outputs": [],
   "source": [
    "#compare with data science courses list with this dataset and keep those only which are in data science list\n",
    "data_science_df=Univ1_graduate_dataset.loc[Univ1_graduate_dataset['course'].isin(data_science_courses_list)]\n",
    "#Include the MATH 661 course in the dataframe\n",
    "data_science_df = data_science_df.append({'course' : 'MATH 661','course_name':'Applied Statistics' ,'course description' : 'Role and purpose of applied statistics. Data visualization and use of statistical software used in course. Descriptive statistics, summary measures for quantitative and qualitative data, data displays. Modeling random behavior: elementary probability and some simple probability distribution models. Normal distribution. Computational statistical inference: confidence intervals and tests for means, variances, and proportions. Linear regression analysis and inference. Control charts for statistical quality control. Introduction to design of experiments and ANOVA, simple factorial design and their analysis.' } , ignore_index=True)\n",
    "#to reset the index \n",
    "data_science_df=data_science_df.reset_index(drop=True)\n",
    "#converting the course_description and course_name  data into string format\n",
    "data_science_df['course description']=data_science_df['course description'].apply(str)\n",
    "data_science_df['course_name']=data_science_df['course_name'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 925,
     "status": "ok",
     "timestamp": 1601396025249,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "I2bd3q2Twb--"
   },
   "outputs": [],
   "source": [
    "#cleansing of data\n",
    "def clean_text(text):\n",
    "    #Convert to lowercase to maintain consistency\n",
    "    text = text.lower()\n",
    "    return text\n",
    "data_science_df['course_name']=data_science_df['course_name'].apply(clean_text)\n",
    "\n",
    "#tokenizing the text\n",
    "def getting_nouns(text):\n",
    "  text_blob_object=TextBlob(text)\n",
    "  return text_blob_object.words\n",
    "\n",
    "data_science_df['words']=data_science_df['course_name'].apply(getting_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 777,
     "status": "ok",
     "timestamp": 1601396035504,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "ocvLDJ_jwyXx"
   },
   "outputs": [],
   "source": [
    "#Genrating the frequent words\n",
    "def gen_freq(text):\n",
    "    #to store the list of words\n",
    "    word_list = []\n",
    "\n",
    "    for tw_words in text:\n",
    "        word_list.extend(tw_words)\n",
    "    #Create word frequencies using word_list\n",
    "    word_freq = pd.Series(word_list).value_counts()\n",
    "    \n",
    "    return word_freq\n",
    "gen_freq(data_science_df['words'])[:50].index\n",
    "#removing stopwords using nltk library\n",
    "data_science_df['words'] = data_science_df['words'].apply(lambda x:[item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "executionInfo": {
     "elapsed": 1046,
     "status": "ok",
     "timestamp": 1601396039519,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "rHBf7v3PxG6j",
    "outputId": "98a737bb-1d45-44d6-ecf5-605e8bfa1f71",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#converting the words column into a single list\n",
    "list_of_topics=data_science_df['words'].tolist()\n",
    "#converting into a single list\n",
    "merged = list(itertools.chain.from_iterable(list_of_topics))\n",
    "#removing the duplicate words\n",
    "data_science_top_100_topics=list(dict.fromkeys(merged))\n",
    "len(merged)\n",
    "len(data_science_top_100_topics)\n",
    "print(*data_science_top_100_topics,sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "executionInfo": {
     "elapsed": 923,
     "status": "ok",
     "timestamp": 1601396041876,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "LG5GROn7xnhj",
    "outputId": "6e90542a-70be-4eef-8af4-5344b926e91a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Update function for feature vector update at a node\n",
    "\n",
    "def update_feature_vector(feature_vector,topic_vector,course_list):\n",
    "  for i in course_list[0]:\n",
    "    for j in range(len(topic_vector)):\n",
    "      if i==topic_vector[j]:\n",
    "         feature_vector[j]=1\n",
    "  return feature_vector \n",
    "\n",
    "#creating feature dictionary with course as key and course topics as value\n",
    "feature_ds_dict={}\n",
    "course_list_ds=list(data_science_df['course'])\n",
    "for i in tqdm(course_list_ds):\n",
    "  temp = list(data_science_df[data_science_df['course']== i ]['words'])\n",
    "  feature_vector_ds =[0]*len(data_science_top_100_topics)\n",
    "  feature_vector_ds = update_feature_vector(feature_vector_ds,data_science_top_100_topics,temp)\n",
    "  feature_ds_dict[i]=feature_vector_ds\n",
    "\n",
    "print(len(course_list_ds))\n",
    "\n",
    "#printing the dictionary\n",
    "'''for i,j in feature_ds_dict.items():\n",
    "  print('{} : {}'.format(i,j))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 3918,
     "status": "ok",
     "timestamp": 1601396049318,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "8_79mJxn1Pa9",
    "outputId": "01a260eb-463b-47da-d997-cf6e0ab71908",
    "tags": []
   },
   "outputs": [],
   "source": [
    "edges=[]\n",
    "for course in data_science_courses_list:\n",
    "  #print(course)\n",
    "  if (data_science_prereq_dict[course]==[]):\n",
    "    CourseId=data_science_courses_list[:]\n",
    "    CourseId.remove(course)\n",
    "    #print(CourseId)\n",
    "    for i in CourseId:\n",
    "      # print(course , (i,course))\n",
    "      edges.append((i,course))\n",
    "  else:\n",
    "      if (len(data_science_prereq_dict[course])!=1 or type(data_science_prereq_dict[course][0])==str):\n",
    "        for j in data_science_prereq_dict[course]:\n",
    "          if(j== [] or j==course):\n",
    "            continue\n",
    "          else:\n",
    "            # print(course , (j,course))\n",
    "            edges.append((j, course)) \n",
    "      else:\n",
    "        l = list(permutations(data_science_prereq_dict[course][0])) \n",
    "        for k in l:\n",
    "          m=0\n",
    "          while (m in range(len(k)-1)):\n",
    "            if ((k[m],k[m+1]) in edges)==False:\n",
    "              edges.append((k[m],k[m+1]))\n",
    "            m+=1\n",
    "          if ((k[len(k)-1],course) in edges)==False:   \n",
    "            edges.append((k[len(k)-1],course))\n",
    "\n",
    "\n",
    "\n",
    "for course in data_science_courses_list:\n",
    "  if(data_science_prereq_dict[course]==[]):\n",
    "    edges.append(('initial' , course))\n",
    "# print(CourseId)\n",
    "# print(edges)\n",
    "       \n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(edges) \n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw_networkx_nodes(G,pos, cmap=plt.get_cmap('jet'),node_size = 1000,node_color=\"lightblue\")\n",
    "nx.draw_networkx_edges(G,pos,edge_color='b', edge_cmap=plt.cm.Blues,arrows=True,arrowstyle=\"->\",arrowsize=10)\n",
    "nx.draw_networkx_labels(G,pos)\n",
    "\n",
    "plt.show()\n",
    "#pylab.show()\n",
    "print(len(edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 871,
     "status": "ok",
     "timestamp": 1601396052364,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "kEr6mjsS1vqs"
   },
   "outputs": [],
   "source": [
    "#creating a movie id dictionary with key as course and node number as value\n",
    "movies = list(G.nodes())\n",
    "movie_ids={}\n",
    "for i,node in enumerate(G.nodes()):\n",
    "  movie_ids[node]=i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 479,
     "status": "ok",
     "timestamp": 1601396053826,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "T8Cyi3xwyLjF"
   },
   "outputs": [],
   "source": [
    "#creating the topic dictonary with key as course name and topic as value\n",
    "topic_dict={}\n",
    "course_topics=list(data_science_df['words'])\n",
    "for i in range(len(course_list_ds)):\n",
    "  topic_dict[course_list_ds[i]]=course_topics[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "executionInfo": {
     "elapsed": 854,
     "status": "ok",
     "timestamp": 1601396055826,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "VxS2YMgTzgLd",
    "outputId": "4327af0d-d15a-4847-883a-b955510d8388",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#printing the topic_dict \n",
    "for i,j in topic_dict.items():\n",
    "  print('{} : {}'.format(i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1117,
     "status": "ok",
     "timestamp": 1601396059528,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "5pAd6cMWz4GA"
   },
   "outputs": [],
   "source": [
    "#creating the input sequence for omega \n",
    "a=random.sample(data_science_core_list,5)\n",
    "b=random.sample(data_science_elective_list,5)\n",
    "def shuffle(a,b):\n",
    "  list_of_sequences=[]\n",
    "  sequence_list=[]\n",
    "  i=0\n",
    "  while i<1000:\n",
    "    sequence_list=[i] + list(a + b)\n",
    "    list_of_sequences.append(sequence_list)\n",
    "    #to randomly select the 5 core courses from the list\n",
    "    a=random.sample(a,5)\n",
    "    #to randomly the select 5 elective courses from the list\n",
    "    b=random.sample(data_science_elective_list,5)\n",
    "    sequence_list=[]\n",
    "    i=i+1\n",
    "  return list_of_sequences\n",
    "\n",
    "list_sequences=shuffle(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKQfRzzOMWD4"
   },
   "source": [
    "Omega **Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "executionInfo": {
     "elapsed": 6172,
     "status": "ok",
     "timestamp": 1601396068096,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "sm1Pjd1yL9iH",
    "outputId": "d1864541-cfa3-48b0-c451-86f407cfad96",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split the sequence of a user into two sequences one to add to the graph and the other to use it as ground truth for evaluation\n",
    "def divide_test(test_user_sequence):\n",
    "    user_id = test_user_sequence[0]\n",
    "    items_in_sequence = test_user_sequence[1:]\n",
    "    l = len(items_in_sequence)//2\n",
    "    previous = items_in_sequence[:l]\n",
    "\n",
    "    left = items_in_sequence[l:]\n",
    "\n",
    "    return user_id , previous , left\n",
    "\n",
    "def compute_edges(gra, sequence):\n",
    "    #start_time = time.time()\n",
    "    SG= nx.DiGraph()\n",
    "    SG.add_nodes_from(sequence)\n",
    "\n",
    "    x=gra.subgraph(sequence)\n",
    "\n",
    "    for i in range(0,len(sequence)):\n",
    "        for p in range(i, len(sequence)):\n",
    "            if (sequence[i],sequence[p]) in x.edges():\n",
    "                SG.add_edge(sequence[i],sequence[p], weight= x[sequence[i]][sequence[p]]['weight'])\n",
    "    #print(' compute edges seconds', time.time() - start_time)\n",
    "    return SG\n",
    "    \n",
    "\n",
    "def somme(arcs):\n",
    "    l = list(arcs.edges())\n",
    "    sum=0\n",
    "    for tup in l:\n",
    "        sum=sum + arcs[tup[0]][tup[1]]['weight']\n",
    "    return sum\n",
    "\n",
    "def utility_sum(gra,sequence):\n",
    "    return (somme(compute_edges(gra,sequence)))\n",
    "\n",
    "# For the conditional variant of OMEGA\n",
    "def noeuds(E,S):   # E is a set of tuples (l), S is a list of items (nodes)\n",
    "\n",
    "    a, b = zip(*E)\n",
    "    S =[ i for i in S ]\n",
    "    l = list(a)+list(b)\n",
    "    y= list(set(l))\n",
    "    z= list(set(y)-set(S))\n",
    "\n",
    "    return z\n",
    "\n",
    "\n",
    "def noeuds1(E):\n",
    "    a,b = zip(*E)\n",
    "    l = list(a)+list(b)\n",
    "    y= list(set(l))\n",
    "    return y\n",
    "\n",
    "def Union(E,e):\n",
    "    return E.union(e)\n",
    "\n",
    "def Union_List(lst1, lst2): \n",
    "    final_list = list(set(lst1) | set(lst2)) \n",
    "    return len(final_list)\n",
    "\n",
    "###### REORDER\n",
    "\n",
    "def REORDER(gra,structure,E,S):  # Compute sequence of items from a set of  edges E according to a graph gra\n",
    "    #np.random.seed(123)  # to have for every call of the function the same topological structure\n",
    "    # works in  k.log(k) |A|=k\n",
    "\n",
    "    A = noeuds(E,S)\n",
    "    #start_time = time.time()\n",
    "    b= sorted(A, key=lambda x: structure.index(x))\n",
    "    #print(' REORDER seconds', time.time() - start_time)\n",
    "\n",
    "    return b\n",
    "\n",
    "\n",
    "def REORDER1(gra,structure,E):  # Compute sequence of items from a set of  edges E according to a graph gra\n",
    "    #np.random.seed(123)  # to have for every call of the function the same topological structure\n",
    "    # works in  k.log(k) |A|=k\n",
    "\n",
    "    A = noeuds1(E)\n",
    "    #start_time = time.time()\n",
    "    b= sorted(A, key=lambda x: structure.index(x))\n",
    "    #print(' REORDER seconds', time.time() - start_time)\n",
    "\n",
    "    return b\n",
    "\n",
    "# OMEGA\n",
    "def OMEGA(gra,structure,  k,S):\n",
    "\n",
    "    edges = set()\n",
    "    arcs = set(gra.edges()) # set of (edges) tuples [ (0,1) , (1,2),....   ]\n",
    "    #print(len(arcs))\n",
    "    # dif = [x for x in arcs if x not in edges]\n",
    "    dif = arcs-edges\n",
    "    C=[]    #  implementing the set C and while condition\n",
    "    for e in dif:\n",
    "        #print(Union( edges,[e]))\n",
    "\n",
    "        if len(noeuds(Union( edges,[e]) ,S)) <= k:\n",
    "            C.append(e)\n",
    "    #utilities.append (utility(gra, REORDER(gra,Union(edges,[e]))))\n",
    "\n",
    "\n",
    "    while C :\n",
    "        maximum = -1   #searching in C the edge that will maximizes the utility function\n",
    "        for i in C:\n",
    "            value  = utility_sum(gra, REORDER1(gra,structure, Union(edges,[i]) ) )\n",
    "            #print('utility', value)\n",
    "            if maximum < value:\n",
    "                maximum= value\n",
    "                tup= i #the edge to add with the maximum utility\n",
    "        \n",
    "        edges.add(tup)\n",
    "\n",
    "#print(edges)\n",
    "        if len(noeuds(edges,S )) ==k:\n",
    "                return  REORDER( gra,structure, edges,S  )\n",
    "\n",
    "        dif = arcs-edges\n",
    "       \n",
    "   #implementing the set C and while condition\n",
    "        C=set()\n",
    "        \n",
    "\n",
    "        for e in dif:\n",
    "\n",
    "            if len(noeuds(Union( edges,[e]),S )) <= k:\n",
    "               C.add(e)\n",
    "\n",
    "#print(C)\n",
    "    return  REORDER( gra,structure, edges,S  )\n",
    "\n",
    "\n",
    "def Precision_k(gra,structure, test_user_sequence, k):\n",
    "    \n",
    "    user_id, previous, left = divide_test(test_user_sequence)\n",
    "    reco = OMEGA(gra,structure,k, previous)\n",
    "    reco =[str(i) for i in reco]\n",
    "    \n",
    "    print(reco)\n",
    "    intersection = list(set(reco) & set(left) )\n",
    "    #print(intersection)\n",
    "    return (len (intersection)/k)\n",
    "\n",
    "\n",
    "\n",
    "# Our Graph Creation for a sequence (  one user  )\n",
    "def Create_OUR(test_user_sequence,z):\n",
    "\n",
    "    transitions = pk.load(open('final_matrix2', 'rb'))\n",
    "    user_id, previous, left = divide_test(test_user_sequence)\n",
    "    candidate_items = list(set(movies).difference(previous))\n",
    "    #candidate_items = left\n",
    "    gra = nx.DiGraph()\n",
    "    for j in previous:\n",
    "        gra.add_node(j)\n",
    "\n",
    "    for i in candidate_items:\n",
    "\n",
    "        gra.add_node(i)\n",
    "        gra.add_edge(i,i)\n",
    "        index_i = movie_ids[i]\n",
    "        gra[i][i]['weight'] = transitions[index_i, index_i]\n",
    "\n",
    "    last_items = previous[-z:]\n",
    "    for item in last_items:\n",
    "        index_last = movie_ids[item]\n",
    "\n",
    "        for i in candidate_items:\n",
    "            index_item = movie_ids[i]\n",
    "            gra.add_edge(item,i)\n",
    "            gra[item][i]['weight'] = transitions[ index_last,index_item]\n",
    "\n",
    "    return gra\n",
    "\n",
    "\n",
    "def worker( argument, results):\n",
    "    \n",
    "    G= nx.DiGraph()\n",
    "\n",
    "    G = Create_OUR(argument,2)\n",
    "\n",
    "    #print(G.edges(data=True))\n",
    "\n",
    "    toto = nx.DiGraph()\n",
    "\n",
    "    toto = G.copy()\n",
    "\n",
    "    toto.remove_edges_from(nx.selfloop_edges(toto,data=True))  # toto\n",
    "\n",
    "    structure = list(nx.topological_sort(toto))  # toto\n",
    "    \n",
    "     \n",
    "    results.append( Precision_k( G,structure,argument,5))\n",
    "\n",
    "\n",
    "random.seed(123)\n",
    "# In the paper, they used 500 sequences at random as a test set\n",
    "# Split sequences into training and testing\n",
    "\n",
    "def make_train_test (sequences, size):\n",
    "\n",
    "    list_shuffled = list_sequences.copy()\n",
    "    #random.shuffle(list_shuffled)\n",
    "    train_data = list_shuffled[size:]\n",
    "    test_data =list_shuffled[:size]\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "##########################\n",
    "def proba(sequences, list_items,topic_dict):\n",
    "    number_of_items = len(list_items)\n",
    "    matrice = np.zeros(shape=(number_of_items,number_of_items))  # to store number of occurences with respect to the threshold l\n",
    "    matrice1 = np.zeros(shape=(number_of_items,number_of_items)) # to store number of occurences\n",
    "\n",
    "    # Version of the probas without the parameter l\n",
    "    for seq in sequences:\n",
    "         items = seq[1:]\n",
    "         for i in range(len(items)-1):\n",
    "            for j in range(i+1, len(items)):\n",
    "                \n",
    "                index_i = list_items[items[i]]\n",
    "                \n",
    "                index_j = list_items[items[j]]\n",
    "                matrice[index_i][index_j] += Union_List(topic_dict[items[i]],topic_dict[items[j]])\n",
    "    \n",
    "    return matrice\n",
    "\n",
    "#Counting the occurance of an item in all the lists we have\n",
    "def frequency (train, list_items,topic_dict):\n",
    "    number_of_items = len(list_items)\n",
    "    list_count = [0]* number_of_items\n",
    "\n",
    "    for sequence in train:\n",
    "        items = sequence[1:]\n",
    "        for item in items:\n",
    "          topic=0\n",
    "          if item in list_items:\n",
    "              index_item = list_items.index(item)\n",
    "              topic=len(topic_dict[item]) \n",
    "              list_count[index_item]+=topic\n",
    "\n",
    "    for i in range(len(list_count)):\n",
    "        if list_count[i]<10:\n",
    "            list_count[i]=0\n",
    "\n",
    "    return list_count\n",
    "\n",
    "# Dividing the occurance of each item by the number of lists we have in our set of sequences\n",
    "def empirical_frequency_items(train, list_items,topic_dict):  # train: training set of sequences , items list of item ids\n",
    "\n",
    "    number_of_items = len(list_items)\n",
    "    list_count = [0]* number_of_items\n",
    "\n",
    "    for sequence in train:\n",
    "        items = sequence[1:]\n",
    "        for item in items:\n",
    "          topic=0\n",
    "          if item in list_items:\n",
    "            index_item = list_items.index(item)\n",
    "            topic=len(topic_dict[item]) \n",
    "            list_count[index_item]+=1\n",
    "\n",
    "    for i in range(len(list_count)):\n",
    "        if list_count[i]<10:\n",
    "            list_count[i]=0\n",
    "\n",
    "    return [x/len(train) for x in list_count]\n",
    "\n",
    "\n",
    "def Transition_matrix (list_sequences, movies):\n",
    "\n",
    "    matrice= pk.load(open ('proba_matrix_without_l', 'rb'))\n",
    "    #first we run Proba function to get the proba_matrix_without_l, we store it and loaod it for faster tests\n",
    "\n",
    "    final_matrix = np.zeros(shape= (len(movies), len(movies)))\n",
    "\n",
    "    frequencies = frequency(list_sequences, movies,topic_dict)\n",
    "    emp_freq = empirical_frequency_items(list_sequences, movies,topic_dict)\n",
    "    for i in range(len(movies)):\n",
    "        for j in range(len(movies)):\n",
    "            if i == j:\n",
    "                final_matrix[i,i]= emp_freq[i]\n",
    "                \n",
    "            else :\n",
    "                if frequencies[i] ==0:\n",
    "                    final_matrix[i,j]= 0\n",
    "                else:\n",
    "                    final_matrix[i,j]= matrice[i,j]/frequencies[i]\n",
    "    return final_matrix\n",
    "\n",
    "\n",
    "#####################\n",
    "\n",
    "##########################################\n",
    "#dividing the data into test and train\n",
    "\n",
    "train_data, test_data = make_train_test(list_sequences,50)\n",
    "\n",
    "m = proba(train_data, movie_ids,topic_dict)\n",
    "print('part2')\n",
    "pk.dump(m, open('proba_matrix_without_l','wb'))\n",
    "\n",
    "print('start')\n",
    "final_matrix = Transition_matrix(train_data, movies)\n",
    "pk.dump(final_matrix , open('final_matrix2', 'wb')) \n",
    "\n",
    "start_time = time.time()\n",
    "manager = multiprocessing.Manager()\n",
    "results = manager.list()\n",
    "\n",
    "jobs=[]\n",
    "for i in range(50):\n",
    "        p = multiprocessing.Process(target = worker, args=(test_data[i], results))\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "\n",
    "for proc in jobs:\n",
    "    proc.join()\n",
    "print ('sum(results)/len(results)',sum(results)/len(results))\n",
    "print('whole', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKs7i3hYLQDm"
   },
   "outputs": [],
   "source": [
    "test_sequence=list_sequences[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpjBHssJUrpM"
   },
   "outputs": [],
   "source": [
    "test_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OM-r3qQpXVq4"
   },
   "outputs": [],
   "source": [
    "#load the results obtained from the omega\n",
    "data=open('omega_results_ds.txt','r')\n",
    "\n",
    "data_2=data.readlines()\n",
    "results_sequences=[]\n",
    "for i in range(50):\n",
    "  \n",
    "  line=data_2[i]\n",
    "  line_1=ast.literal_eval(line)\n",
    "  line_2=[i] + line_1\n",
    "\n",
    "  line_3=line_2\n",
    "  results_sequences.append(line_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7IHof4tCzDm"
   },
   "outputs": [],
   "source": [
    "#creating final results sequence with previous sequence from the input sequence with sequence obtained from the results of omega framework\n",
    "final_sequence=[]\n",
    "for i in range(len(test_sequence)):\n",
    "  list_1=test_sequence[i]\n",
    "  list_2=list_1[1:6]\n",
    "  list_3=results_sequences[i]\n",
    "  list_4=list_3[1:]\n",
    "  final_sequence.append(list_2 + list_4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "executionInfo": {
     "elapsed": 390,
     "status": "ok",
     "timestamp": 1599529395951,
     "user": {
      "displayName": "Baljinder Smagh",
      "photoUrl": "",
      "userId": "16670751434212058453"
     },
     "user_tz": 240
    },
    "id": "PZ_FeINPDSBW",
    "outputId": "12c8535e-1b10-4f64-dfe5-77ee8090ab91"
   },
   "outputs": [],
   "source": [
    "print(*final_sequence,sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kb8aaBwADwCZ"
   },
   "outputs": [],
   "source": [
    "#saving the results\n",
    "with open('test_results_ds.txt', 'w') as f:\n",
    "    for item in final_sequence:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNTW4UQ34PhYP9p5Wu2BaRr",
   "mount_file_id": "1TuYpbPmS1oeDwn1gxdi8_Q4iiLU5ne3L",
   "name": "Omega_DS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}